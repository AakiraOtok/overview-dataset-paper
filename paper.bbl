\begin{thebibliography}{10}

\bibitem{ILSVRC}
O.~Russakovsky, J.~Deng, H.~Su, J.~Krause, S.~Satheesh, S.~Ma, Z.~Huang,
  A.~Karpathy, A.~Khosla, M.~Bernstein, A.~C. Berg, and L.~Fei-Fei, ``{ImageNet
  Large Scale Visual Recognition Challenge},'' {\em International Journal of
  Computer Vision (IJCV)}, vol.~115, no.~3, pp.~211--252, 2015.

\bibitem{PASCALVOC}
M.~Everingham, L.~Van~Gool, C.~Williams, J.~Winn, and A.~Zisserman, ``The
  pascal visual object classes (voc) challenge,'' {\em International Journal of
  Computer Vision}, vol.~88, pp.~303--338, 06 2010.

\bibitem{LeNet}
Y.~Lecun, L.~Bottou, Y.~Bengio, and P.~Haffner, ``Gradient-based learning
  applied to document recognition,'' {\em Proceedings of the IEEE}, vol.~86,
  no.~11, pp.~2278--2324, 1998.

\bibitem{KTH}
C.~Schuldt, I.~Laptev, and B.~Caputo, ``Recognizing human actions: a local svm
  approach,'' in {\em Proceedings of the 17th International Conference on
  Pattern Recognition, 2004. ICPR 2004.}, vol.~3, pp.~32--36 Vol.3, 2004.

\bibitem{Weizmann}
M.~Blank, L.~Gorelick, E.~Shechtman, M.~Irani, and R.~Basri, ``Actions as
  space-time shapes,'' in {\em Tenth IEEE International Conference on Computer
  Vision (ICCV'05) Volume 1}, vol.~2, pp.~1395--1402 Vol. 2, 2005.

\bibitem{IXMAS}
D.~Weinland, E.~Boyer, and R.~Ronfard, ``Action recognition from arbitrary
  views using 3d exemplars,'' in {\em 2007 IEEE 11th International Conference
  on Computer Vision}, pp.~1--7, 2007.

\bibitem{HollyWood}
I.~Laptev, M.~Marszalek, C.~Schmid, and B.~Rozenfeld, ``Learning realistic
  human actions from movies,'' in {\em 2008 IEEE Conference on Computer Vision
  and Pattern Recognition}, pp.~1--8, 2008.

\bibitem{UCFSports}
M.~D. Rodriguez, J.~Ahmed, and M.~Shah, ``Action mach a spatio-temporal maximum
  average correlation height filter for action recognition,'' in {\em 2008 IEEE
  Conference on Computer Vision and Pattern Recognition}, pp.~1--8, 2008.

\bibitem{Hollywood2}
M.~Marszalek, I.~Laptev, and C.~Schmid, ``Actions in context,'' in {\em 2009
  IEEE Conference on Computer Vision and Pattern Recognition}, pp.~2929--2936,
  2009.

\bibitem{UCFYouTube}
J.~Liu, J.~Luo, and M.~Shah, ``Recognizing realistic actions from videos “in
  the wild”,'' in {\em 2009 IEEE Conference on Computer Vision and Pattern
  Recognition}, pp.~1996--2003, 2009.

\bibitem{Olympic}
J.~C. Niebles, C.-W. Chen, and L.~Fei-Fei, ``Modeling temporal structure of
  decomposable motion segments for activity classification,'' in {\em Computer
  Vision -- ECCV 2010} (K.~Daniilidis, P.~Maragos, and N.~Paragios, eds.),
  (Berlin, Heidelberg), pp.~392--405, Springer Berlin Heidelberg, 2010.

\bibitem{HMDB51}
H.~Kuehne, H.~Jhuang, E.~Garrote, T.~Poggio, and T.~Serre, ``Hmdb: A large
  video database for human motion recognition,'' in {\em 2011 International
  Conference on Computer Vision}, pp.~2556--2563, 2011.

\bibitem{UCF101}
K.~Soomro, A.~R. Zamir, and M.~Shah, ``Ucf101: A dataset of 101 human actions
  classes from videos in the wild,'' 2012.

\bibitem{Sports1M}
A.~Karpathy, G.~Toderici, S.~Shetty, T.~Leung, R.~Sukthankar, and L.~Fei-Fei,
  ``Large-scale video classification with convolutional neural networks,'' in
  {\em CVPR}, 2014.

\bibitem{ActivityNet}
F.~C. Heilbron, V.~Escorcia, B.~Ghanem, and J.~C. Niebles, ``Activitynet: A
  large-scale video benchmark for human activity understanding,'' in {\em 2015
  IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pp.~961--970, 2015.

\bibitem{THUMOS}
H.~Idrees, A.~R. Zamir, Y.-G. Jiang, A.~Gorban, I.~Laptev, R.~Sukthankar, and
  M.~Shah, ``The thumos challenge on action recognition for videos “in the
  wild”,'' {\em Computer Vision and Image Understanding}, vol.~155,
  p.~1–23, Feb. 2017.

\bibitem{YouTube8M}
S.~Abu-El-Haija, N.~Kothari, J.~Lee, P.~Natsev, G.~Toderici, B.~Varadarajan,
  and S.~Vijayanarasimhan, ``Youtube-8m: A large-scale video classification
  benchmark,'' 2016.

\bibitem{Charades}
G.~A. Sigurdsson, G.~Varol, X.~Wang, A.~Farhadi, I.~Laptev, and A.~Gupta,
  ``Hollywood in homes: Crowdsourcing data collection for activity
  understanding,'' 2016.

\bibitem{Multi-THUMOS}
S.~Yeung, O.~Russakovsky, N.~Jin, M.~Andriluka, G.~Mori, and L.~Fei-Fei,
  ``Every moment counts: Dense detailed labeling of actions in complex
  videos,'' {\em International Journal of Computer Vision}, 2017.

\bibitem{Kinetics-400}
W.~Kay, J.~Carreira, K.~Simonyan, B.~Zhang, C.~Hillier, S.~Vijayanarasimhan,
  F.~Viola, T.~Green, T.~Back, P.~Natsev, {\em et~al.}, ``The kinetics human
  action video dataset,'' {\em arXiv preprint arXiv:1705.06950}, 2017.

\bibitem{Kinetics-600}
J.~Carreira, E.~Noland, A.~Banki-Horvath, C.~Hillier, and A.~Zisserman, ``A
  short note about kinetics-600,'' {\em arXiv preprint arXiv:1808.01340}, 2018.

\bibitem{Kinetics-700}
J.~Carreira, E.~Noland, C.~Hillier, and A.~Zisserman, ``A short note on the
  kinetics-700 human action dataset,'' {\em arXiv preprint arXiv:1907.06987},
  2019.

\bibitem{Kinetics-700-2020}
L.~Smaira, J.~Carreira, E.~Noland, E.~Clancy, A.~Wu, and A.~Zisserman, ``A
  short note on the kinetics-700-2020 human action dataset,'' {\em arXiv
  preprint arXiv:2010.10864}, 2020.

\bibitem{AVA}
C.~Gu, C.~Sun, D.~A. Ross, C.~Vondrick, C.~Pantofaru, Y.~Li,
  S.~Vijayanarasimhan, G.~Toderici, S.~Ricco, R.~Sukthankar, C.~Schmid, and
  J.~Malik, ``Ava: A video dataset of spatio-temporally localized atomic visual
  actions,'' 2018.

\bibitem{MomentsinTime}
M.~Monfort, A.~Andonian, B.~Zhou, K.~Ramakrishnan, S.~A. Bargal, T.~Yan,
  L.~Brown, Q.~Fan, D.~Gutfruend, C.~Vondrick, and A.~Oliva, ``Moments in time
  dataset: one million videos for event understanding,'' 2019.

\bibitem{HACS}
H.~Zhao, A.~Torralba, L.~Torresani, and Z.~Yan, ``Hacs: Human action clips and
  segments dataset for recognition and temporal localization,'' 2019.

\bibitem{HVU}
A.~Diba, M.~Fayyaz, V.~Sharma, M.~Paluri, J.~Gall, R.~Stiefelhagen, and L.~V.
  Gool, ``Large scale holistic video understanding,'' 2020.

\bibitem{AViD}
A.~Piergiovanni and M.~S. Ryoo, ``Avid dataset: Anonymized videos from diverse
  countries,'' 2020.

\bibitem{FineAction}
Y.~Liu, L.~Wang, Y.~Wang, X.~Ma, and Y.~Qiao, ``Fineaction: A fine-grained
  video dataset for temporal action localization,'' {\em IEEE Transactions on
  Image Processing}, 2022.

\bibitem{ToyotaSmarthome}
S.~Das, R.~Dai, M.~Koperski, L.~Minciullo, L.~Garattoni, F.~Bremond, and
  G.~Francesca, ``Toyota smarthome: Real-world activities of daily living,'' in
  {\em Proceedings of the IEEE/CVF international conference on computer
  vision}, pp.~833--842, 2019.

\bibitem{SomethingSomething}
R.~Goyal, S.~E. Kahou, V.~Michalski, J.~Materzyńska, S.~Westphal, H.~Kim,
  V.~Haenel, I.~Fruend, P.~Yianilos, M.~Mueller-Freitag, F.~Hoppe, C.~Thurau,
  I.~Bax, and R.~Memisevic, ``The "something something" video database for
  learning and evaluating visual common sense,'' 2017.

\bibitem{EPICKITCHENS100}
D.~Aldamen, D.~Moltisanti, E.~Kazakos, H.~Doughty, J.~Munro, W.~Price, M.~Wray,
  T.~Perrett, and J.~Ma, ``Epic-kitchens-100,'' 2020.

\bibitem{VideoMAEV2g}
L.~Wang, B.~Huang, Z.~Zhao, Z.~Tong, Y.~He, Y.~Wang, Y.~Wang, and Y.~Qiao,
  ``Videomae v2: Scaling video masked autoencoders with dual masking,'' 2023.

\bibitem{ip-CSN-152}
D.~Tran, H.~Wang, L.~Torresani, and M.~Feiszli, ``Video classification with
  channel-separated convolutional networks,'' 2019.

\bibitem{TriDet}
D.~Shi, Q.~Cao, Y.~Zhong, S.~An, J.~Cheng, H.~Zhu, and D.~Tao, ``Temporal
  action localization with enhanced instant discriminability,'' 2023.

\bibitem{AdaTAD}
S.~Liu, C.-L. Zhang, C.~Zhao, and B.~Ghanem, ``End-to-end temporal action
  detection with 1b parameters across 1000 frames,'' 2023.

\bibitem{Text4Vis}
W.~Wu, Z.~Sun, and W.~Ouyang, ``Revisiting classifier: Transferring
  vision-language models for video recognition,'' 2023.

\bibitem{TTM}
M.~S. Ryoo, K.~Gopalakrishnan, K.~Kahatapitiya, T.~Xiao, K.~Rao, A.~Stone,
  Y.~Lu, J.~Ibarz, and A.~Arnab, ``Token turing machines,'' 2023.

\bibitem{TokenLearner}
M.~S. Ryoo, A.~Piergiovanni, A.~Arnab, M.~Dehghani, and A.~Angelova,
  ``Tokenlearner: What can 8 learned tokens do for images and videos?,'' 2022.

\bibitem{InternVideo}
Y.~Wang, K.~Li, Y.~Li, Y.~He, B.~Huang, Z.~Zhao, H.~Zhang, J.~Xu, Y.~Liu,
  Z.~Wang, S.~Xing, G.~Chen, J.~Pan, J.~Yu, Y.~Wang, L.~Wang, and Y.~Qiao,
  ``Internvideo: General video foundation models via generative and
  discriminative learning,'' 2022.

\bibitem{TubeVit-H}
A.~Piergiovanni, W.~Kuo, and A.~Angelova, ``Rethinking video vits: Sparse video
  tubes for joint image and video learning,'' 2022.

\bibitem{DCGN}
F.~Mao, X.~Wu, H.~Xue, and R.~Zhang, {\em Hierarchical Video Frame Sequence
  Representation with Deep Convolutional Graph Network}, p.~262–270.
\newblock Springer International Publishing, 2019.

\bibitem{MVD}
R.~Wang, D.~Chen, Z.~Wu, Y.~Chen, X.~Dai, M.~Liu, L.~Yuan, and Y.-G. Jiang,
  ``Masked video distillation: Rethinking masked feature modeling for
  self-supervised video representation learning,'' 2023.

\bibitem{LART}
J.~Rajasegaran, G.~Pavlakos, A.~Kanazawa, C.~Feichtenhofer, and J.~Malik, ``On
  the benefits of 3d pose and tracking for human action recognition,'' 2023.

\bibitem{UMT-L}
S.~Srivastava and G.~Sharma, ``Omnivec: Learning robust representations with
  cross modal sharing,'' 2023.

\bibitem{UniFormerV2-L}
K.~Li, Y.~Wang, Y.~He, Y.~Li, Y.~Wang, L.~Wang, and Y.~Qiao, ``Uniformerv2:
  Spatiotemporal learning by arming image vits with video uniformer,'' 2022.

\bibitem{MS-TCT}
R.~Dai, S.~Das, K.~Kahatapitiya, M.~S. Ryoo, and F.~Bremond, ``Ms-tct:
  Multi-scale temporal convtransformer for action detection,'' 2021.

\bibitem{PAAT}
D.~Reilly, A.~Chadha, and S.~Das, ``Seeing the pose in the pixels: Learning
  pose-aware representations in vision transformers,'' 2023.

\bibitem{Avion}
Y.~Zhao and P.~Krähenbühl, ``Training a large video model on a single machine
  in a day,'' 2023.

\end{thebibliography}
